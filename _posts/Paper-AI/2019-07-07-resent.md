---
layout: post
title: Classification [1] ResNet
excerpt: AI Paper review_ResNet
comments: true
categories : [AI Paper Review, ResNet]
use_math: true
---


### 1. Convolutional Neural Network의 문제
기존의 Convolutional Neural Network(이하 CNN)의 문제점은 네트워크의 깊이가 깊어질수록 학습을 진행하는 것이 어렵다는 단점이 있습니다. AlexNet은 8층, VGC는 19층, GooleNet은 22층에 불과했으며, 많은 연구들이 20층 내외의 층에서 실험을 진행했습니다. 왜 그랬을까요?

#### __1.1 Vanishing/Exploding Gradient__<br>
손실함수(Loss Function 혹은 Cost Function)을 최소화 하기 위해 우리는 학습률(Learning rate)을 설정합니다. 많이 보았던 수식입니다.

$$
    W^* \leftarrow W - \eta\frac{\partial L}{\partial W}
$$

위 수식에서 $\eta$(에타)가 학습률이 되고, 관례적으로 1e-2 ~ 1e-4 사이의 값으로 지정해주었습니다. $\eta$를 너무 작게 설정하면 $\frac{\partial L}{\partial W}$가 0으로 수렴해버려서 Vanishing 된다고 합니다. 반면 $\eta$를 크게 설정하면 발산해버려서 Exploding 된다고 합니다. 신경망의 층이 깊어질 수록 이를 제어하기가 어려웠던것이죠.

#### __1.2 망이 깊어지면서 Error가 증가(이하 Degradation)__<br>
ResNet의 저자인 Kaiming He의 논문에는 Degradation Problem을 다루었습니다. 그림과 같이 20층인 신경망에서보다 56층에서의 신경망의 Tranning, Test Error 모두 높은것을 알 수 있습니다. 왜 이러한 현상이 발생하는지는 아직 밝혀내지 못하였습니다.

### 2. ResNet

이 두가지를 개선시킨 알고리즘이 바로 **ResNet[^0]**입니다. ResNet은 Residual Network의 약자로 __잔차__ 의 개념을 도입한 방법입니다. 이를 이해하기 위해서는 우선 Block의 개념과 Identity Mapping이라는 것을 알아야 합니다. 

#### 2.1 Block

![34layer](https://www.dropbox.com/s/egpvgpitr109lak/34layer_resnet.png?raw=1)

Bloack은 layer의 묶음 입니다. 위 그림에서와 같이 Resnet에서는 2개의 Convolution Layer를 하나의 Block으로 묶습니다. ResNet의 Residual의 이름을 따서 이를 Residual Block이라고 부르기도 합니다. 그림을 잘 살펴보면 첫번째 Block의 Convolution Layer의 필터크기는 3x3이고 갯수는 64개 입니다. 이 Block의 연산량을 살펴보면 3x3x64의 Convolution Layer가 2개가 있으므로 3x3x64+3x3x64 = 1,152 입니다. 이 값은 앞쪽의 Layer에서 받은 채널이 1일때로 최소값입니다. 층이 깊어질수록 이 값은 기하급수적으로 늘어나게되겠죠. 그래서 저자인 Kaiming He는 Bottleneck Block이라는것을 제안합니다.

<p align="center">
<img alt="bottleneck" src="https://www.dropbox.com/s/tgtmn1dpuovptrk/bottlenect.png?raw=1">
</p>

34개, 50개의 Convolution Layer를 가진 ResNet입니다. 일단 깊이가 34냐 50이냐는 차치해두고 구조를 살펴보겠습니다. 이전의 Layer에서 입력으로 들어온 채널은 256입니다. 왼쪽 Block의 연산량은 3x3x256x256+3x3x256x256 = 393,216 입니다. 오른쪽의 Bottleneck Block의 연산량은 1x1x256x64+3x3x64x64+1x1x64x256 = 69,632 입니다. 연산량에서 엄청난 간소화가 가능해졌으면서도 왼쪽과 오른쪽은 비슷한 복잡성을 **보이고** 있기 때문에 대체가능하다라고 설명하고 있습니다. 이와같이 차원을 줄였다가 다시 늘리는것이 마치 병목과 같다 하여 Bottleneck Block이라고 합니다.

#### 2.2 Ientity Mapping

Identity Mapping이란 Identity Function $h(x)=x$를 만족하도록 하는것입니다. 더 간단히 설명하면, 입력으로 들어간 미지의 값 $x$가 어떠한 함수$h$를 통과한 후에도 다시 $x$가 나와야 한다는 것이죠.

<p align="center">
<img alt="plain" src="https://www.dropbox.com/s/u8xa3kfgseh28f8/resnet_plain.png?raw=1">
</p>


위 그림은 일반적인 CNN의 구조입니다. 입력으로 들어온 $x$가 weight layer(convolution layer와 같은말)를 지나가고 활성함수 relu를 통과하는 형태입니다. 우리는 이렇게 통과하고 나온 최종적인 형태인 $H(x)$를 최적화 하는것을 목표로 삼았습니다.

<p align="center">
<img width="450" alt="resnet" src="https://www.dropbox.com/s/6fqivraxi0zlvpo/resnet_resnet.png?raw=1">
</p>

위 그림이 바로 ResNet의 구조입니다. 입력으로 들어온 $x$를 weight layer을 통해 아래로 흘러가는것은 동일합니다. 차이점은 Identity라고 하는 연결이 추가되었다는것 입니다.(Short Connection 혹은 Skip Connection이라고도 합니다.) 아래로 흘러 내려온 부분은 $F(x)$라 한다면 $H(x)=F(x) + h(x)$가 되고, Identity Function에 의해 $h(x)=x$가 되어 $H(x)=F(x) + x$가 됩니다. 이렇게 나온 출력 $H(x)$는 다음 레이어의 인풋으로 들어가게 되겠죠. 해당구조를 조금더 자세히 알아보기위해 아래의 수식을 살펴보겠습니다.

$$
    y_{l} = h(x_{l}) + F(x_{l},W_{l}) = x_{l} + F(x_{l},W_{l})\\
    f(y_{l}) = x_{l+1}
$$

아래첨자 $l$은 Block 단위(Unit)가 됩니다. $h(x)$와 weight layer을 통해 내려온 $F(x)$가 더해져 출력 $y$가 되고 이는 $f$라는 Relu라는 activation function을 통과해 다음레이어의 인풋이 됩니다. 하지만 Kaiming He는 이를 진정한 의미의 Identity Mapping이라고 생각하지 않았습니다. ResNet의 2016년 논문인 **Identity Mapping in Residual Networks[^1]**에서는 이러한 점을 개선한 부분이 등장합니다. 바로 activation function 대신 Identity function을 취해주는것입니다. $H(x)=F(x) + x$식에서 바로 함수$H$를 다시 Identity Function이라고 생각한 것이죠.

$$
    x_{l+1} = h(y_{l}) = h(h(x_{l})) + F(x_{l},W_{l}) = x_{l} + F(x_{l},W_{l})
$$

조금 더 직관적으로 알아보기 위해 $l+2$번째까지 적어보도록 하겠습니다.

$$
    x_{l+2} = h(y_{l+1}) = x_{l+1} + F(x_{l+1},W_{l+1})\\
    \qquad\qquad\qquad\qquad\quad\quad\quad =x_{l} + F(x_{l},W_{l}) + F(x_{l+1},W_{l+1})
$$

이제 이를 하나의 식으로 작성할 수 있습니다.

$$
    x_{L} = x_{l} + \sum_{i=L}^{L-1}F(x_{i},W_{i})
$$

어떠한 $L$번째의 Block의 출력은 초기의 입력받는 층($l$)과 그 이전까지 연결된 층의 **합(SUM)**으로 이루어져 있다는것입니다. 이것은 연산 복잡도에 엄청난 영향을 미치게 됩니다. 역전파를 진행할때 곱에대한 편미분보다 합에 대한 편미분이 훨씬 간단하기 때문이죠. 상세하게 알아보기 위해 위 식을 $x$에 대해 편미분을 진행해 봅시다.<br>
##### *~~수식이 보기 싫으면 이곳은 지나치셔도 됩니다.~~*

$$
    \frac{\partial \varepsilon}{\partial x_{l}} = \frac{\partial \varepsilon}{\partial x_{L}}\frac{\partial x_{L}}{\partial x_{l}} = \frac{\partial \varepsilon}{\partial x_{L}}\left(1+\frac{\partial}{\partial x_{L}}\sum_{i=l}^{L-1}F(x_{i},W_{i})\right) 
$$

$x_{l}$을 Loss Function($\varepsilon$)에 대하여 미분하였을때 $\frac{\partial \varepsilon}{\partial x_{L}}$와 $\frac{\partial x_{L}}{\partial x_{l}}$의 곱으로 표현됩니다. 앞쪽의 $\frac{\partial \varepsilon}{\partial x_{L}}$ L번째 Block단위의 정보를 뜻하고 이것은 $l$번째 Layer에 바로 전파됩니다. 뒷쪽의 $1+\frac{\partial}{\partial x_{L}}\sum_{i=l}^{L-1}F(x_{i},W_{i})$부분은 Loss Function이 있는 Layer에서 부터 $l$번째 Weight Layer까지의 편미분 합이 됩니다. 따라서 연산량도 줄어들 뿐만 아니라 Gredient가 사라지는 Vanishing 문제도 없어지게 되는겁니다.(가중치 초기화 방법인 Xaiver나 He를 생각해보세요. 초기화된 가중치를 모두 더했을때 -1이 나올 확률이 얼마나 있을까요.)

다시 위쪽의 $H(x)=F(x) + x$식을 보겠습니다. 진정한 의미의 Identity Mapping은 $H(x)=F(x) + x = x$가 됩니다.($H$는 Identity Function). Identity의 가정이 성립되기 위해서는 $F(x)$가 0이 되어야합니다. $F(x)=H(x)-x$가 되고 이것은 마치 잔차처럼 보이게됩니다.(마치 선형회귀모형에서 많이 보던 $y=X\beta + \epsilon$의 모습이죠). 그래서 이 알고리즘이 Residual Network가 된것입니다.

Identity Mapping의 개념은 여기까지이지만 더 살펴보아야 할 부분이 있습니다.

#### 2.3 차원의 조정

맨 위쪽의 그림을 자세히 살펴보면 Identity Mapping이 실선으로 되는부분과 점선으로 되어있는 부분이 있습니다. 이것은 도대체 무엇일까요? 바로 차원을 맞춰주기 위해 shortcut이 조정이 되는것이죠.

첫번째 Block에서 Convolution Layer의 필터의 크기는 3x3이고 갯수는 64개 Stride는 1입니다. 하지만 세번째 Block의 마지막 Convolution Layer는 필터의 갯수가 128개이고 Stride가 2가 됩니다. 그래서 첫번째 Block을 통과하고 다음 Block으로 들어갈때 이미지의 크기가 반으로 Down Sampling 됩니다. 예를들어 입력이미지 크기가 224x224라면 첫번째 Block을 통과하게 되면 112x112가 되는것입니다. 따라서 Shortcut connection을 할 수 없게됩니다.

##### Convolution Block

<p align="center">
<img width="600" alt="conv_block" src="https://www.dropbox.com/s/hgaqdywpqy1njrt/convolutional%20block.png?raw=1">
</p>

##### Identity Block

<p align="center">
<img width="600" alt="id_block" src="https://www.dropbox.com/s/vjx717lo18f9s45/identity_block.png?raw=1">
</p>

그래서 이를 맞춰주기위해 Convolution Layer를 통해서 shortcut을 연결해줍니다. 이렇게 차원이 조정되어 연결되는것을 Projection_shortcut이라고하고, Convolution Block이라고 부릅니다. 일반적인 shortcut 형태는 Identity Block이라고 부르게 됩니다.

### 3. Pre-Activation

ResNet 모델에서는 가중치 초기화 방법으로 He, Xaiver와 같이 잘 알려진 초기화 방법을 사용하지 않습니다. 다만 Convolution Layer의 뒤에 Batch Normalization(이하 BN)을 취해주는 방법을 사용합니다. 그리고 BN과 Activation Layer의 순서를 다양한 방법으로 변경하면서 퍼포먼스를 실험해 보았습니다.

<p align="center">
<img width="500" alt="pre_activation" src="https://www.dropbox.com/s/o1i8kciafwf2pif/pre_activation.png?raw=1">
</p>

<p align="center">
<img width="500" alt="pre_activation_graph" src="https://www.dropbox.com/s/rx0xo0c8ffvl92e/pre_activation_graph.png?raw=1">
</p>

기존의 방법인 (a)를 보다 ReLU앞에서 BN을 해준 (e) 방법이 더 뛰어난 퍼포먼스를 보이고있습니다.

Kaiming He는 두가지 이유로 인해 더 좋은 퍼포먼스를 보인다고 얘기하고있습니다.

1. Pre_activation은 역전파를 할 때 Activation 함수인 ReLU에서 truncated될 여지가 없다.
2. Batch_normalization이 ReLu이전에 있기 때문에 입력으로 들어온 데이터정규화(regularization) 측면에서 개선되었다.

ResNet의 이러한 아이디어는 이후에 나오는 논문에서 상당한 영향을 미쳤고, Object Detection에서도 상당부분 참고되고 있습니다.

ResNet의 개념은 이쯤에서 마무리하고 Tensorflow 코드분석을 통해 실제 이미지에 적용해보는 시간을 가져보도록 하겠습니다.

#### Reference

[^0]: https://arxiv.org/pdf/1512.03385.pdf

[^1]: https://arxiv.org/pdf/1603.05027.pdf