---
layout: post
title: Object Detection [5] SSD:Single Shot MultiBox Detector
excerpt: AI Paper Review_SSD
comments: true
categories : [AI Paper Review, SSD, Single Shot MultiBox Detector]
use_math: true
---

YoLo v1이 등장한 이후 약 5달뒤 Single Show Multibox Detector라는 논문이 등장하였습니다. 논문을 이해해보면 알겠지만 Multibox라는 개념이 중요한데, 줄임말이 왜 SSMD가 아니라 SSD인지는 모르겠습니다. 아마 One Stage Method라는것을 더욱 강조하고 싶었던 모양입니다.

해당 논문은 당시 YoLo v1보다 빠르고 Faster R-CNN보다 더 정확한 Detector로 SOTA모델이었으며, 저는 2019년 상반기에 객체검출 프로젝트를 수행할때 해당 모델을 사용하기도 했습니다. 개인적으로는 내용도 참신하고 우수한 모델이라 생각합니다.

### 1. Process

SSD의 Main Idea는 다양한 크기를 가진 Feature Map을 획득하는것입니다. VGG-16의 FC layer를 제외한 conv5-3영역까지의 네트워크 구조를 가져옵니다. 다음으로는 3x3 size를 가진 6개의 Conv Layer를 통해 서로다른 크기를 가지는 Feature Map을 획득하고 각각의 Feture Map으로부터 Bounding Box와 Classification을 진행합니다. 해당 논문에서는 Backbone으로 VGG-16을 활용하였지만 ResNet 등 다른 모델로도 좋은 결과를 낼 수 있다고 합니다. 타 모델 사용시 FC layer가 존재한다면 해당 부분은 제외한 네트워크 구조를 사용하면 됩니다.

<p align="center">
<img width="600" alt="network" src="https://www.dropbox.com/s/vdluj4p8b4kuht7/network.PNG?raw=1">
</p>

SSD는 6개의 Conv Layer를 통해 얻은 각각의 Feature Map들에 대해서 모두 Prediction을 진행합니다. Feature Map들의 size는 점점 작아지는데 이것은 이미지의 grid cell이 점점 커지는것을 의미합니다. 
아래 그림을 살펴보면, 8x8 size의 Feature Map에서는 작은 크기의 Default Box를 사용합니다. 크기가 큰 강아지(Red Ground Truth)는 탐지해내는것이 어렵고 크기가 작은 고양이(Blue Ground Truth)는 탐지해내는것이 유리할것입니다. 반면, 4x4 size의 Feature Map에서는 큰 크기의 Default Box를 사용하기 때문에 큰 객체를 탐지해내는것에 유리하다는것을 알 수 있습니다.

<p align="center">
<img width="600" alt="network" src="https://www.dropbox.com/s/6ww4caohhe4cn2x/feature_map.PNG?raw=1">
</p>

### 2. Default Box

SSD는 Faster R-CNN에서 사용하는 Anchor Box와 비슷한 개념인 Default Box를 사용합니다. 고정된 Box라는 의미로 한번 설정해놓은 Default Box는 불변적이며, 각각의 Feature Map의 수만큼 Default Box를 준비해놓습니다. 좀더 구체적으로 살펴보면:

#### scale

$$
    s_{k} = s_{min} + \frac{s_{max} - s_{min}}{m - 1}(k - 1), \qquad k \in [1,m]\\
$$

$$
    s_{min}=0.2, \quad s_{max}=0.9, \quad m= the\;number\;feature\;maps 
$$

$k$는 Feature Map의 갯수가 $m$개일때의 $k$번째라는것을 나타내는 인덱스 입니다.
$s_{min}\; s_{max}$은 가장 첫번째로 나타나는 Feature Map의 Default Box의 scale은 0.2이고 가장 마지막에 나타나는 Feature Map의 Default Box의 scale은 0.9라는 것입니다. 그리고 사이에 있는 Feature Map들의 Default Box의 scale은 등간격 크기로 설정합니다. 

#### aspect ratio

Default Box마다 각기 다른 aspect ration를 설정해주어야 합니다. 이 부분은 고정값으로 사용합니다.

$$
    a_{r} \in \{ 1, 2, 3, \frac{1}{2}, \frac{1}{3} \}
$$

그리고 aspect ration가 1인 Default Box에 $s^{\prime}_{k} = \sqrt{s_{k}s_{k+1}}$ 크기만큼 확대한 Default Box를 하나 더 추가해서 총 6개의 Box를 생성합니다.

#### width & height

위에서 구한 scale과 aspect ration를 활용하여 값을 계산해줍니다.

$$
    w_{k}^{a} = s_{k}\sqrt{a_{r}}, \quad h_{k}^{a} = \frac{s_{k}}{\sqrt{a_{r}}}
$$

#### center

Default Box의 중심은 Feature Map의 크기로 정규화된 값을 사용합니다.

$$
    (x, y) = (\frac{i+0.5}{\vert f_{k} \vert}, \frac{j+0.5}{\vert f_{k} \vert})
$$

$$
    \vert f_{k} \vert = size\;of\;the\;k-th\;square\;feature\;map \quad i,j \in [0, \vert f_{k} \vert )
$$

이렇게 Feature Map당 6개의 Default Box를 생성하였습니다. 이제 Box를 각각의 위치(grid cell)마다 두고 Ground Truth Box와 매칭하는 작업을 진행합니다.

### 3. Loss Function

SSD의 Loss Function은 Faster R-CNN과 유사한 형태로 구성되어있습니다. 사실 One Stage의 Loss Function은 거의 비슷한 편입니다.

$$
    L(x,c,l,g) = \frac{1}{N}(L_{conf}(x,c) + \alpha L_{loc}(x,l,g))
$$

$N$은 매칭된 Default Box의 갯수입니다. 만약 하나도 매칭된것이 없다면 Loss는 0이 됩니다. $\alpha$는 1로 설정하였다고 합니다.
#### localization loss

localization loss는 여전히 Smooth L1 loss를 사용하고 있습니다. 개념은 Faster R-CNN과 유사한 방식으로 계산됩니다.

$$
    L_{loc}(x,l,g)=\sum_{i \in Pos}^{N} \sum_{m \in \{ cx, cy, w, h \}} x_{ij}^{k}smooth_{L1}(l_{i}^{m} - \hat{g_{j}}^{m})
$$

$$
    \hat{g_{j}}^{cx} = (g_{j}^{cx}-d_{i}^{cx})/d_{i}^{w} \quad \hat{g_{j}}^{cy} = (g_{j}^{cy}-d_{i}^{cy})/d_{i}^{h}\\
    \hat{g_{j}}^{w} = log(\frac{g_{j}^{w}}{d_{i}^{w}}) \quad \hat{g_{j}}^{h} = log(\frac{g_{j}^{h}}{d_{i}^{h}})
$$

Ground Truth Box와 미리 계산해둔 Default Box와 계산하여 $\hat{g_{j}}^{m}$를 획득하고 이를 Predict Box와 계산하여 localizaation loss를 구하게 됩니다.

#### classification loss

마찬가지로 multiple class confidence를 구하기 위해서 softmax loss를 사용해줍니다.

$$
    L_{conf}(x,c) = - \sum_{i \in Pos}^{N}x_{ij}^{p}log(\hat{c_i}^{p}) - \sum_{i \in Neg}log(\hat{c_{i}}^{0}) \qquad where \quad \hat{c_{i}}^{p} = \frac{exp(c_{i}^{p})}{\sum_{p}exp(c_{i}^{p})}
$$

$x_{ij}^{p}$는 p라는 class의 i번째 Default Box와 j번째 Ground Truth Box가 매칭 되었을때는 1, 그렇지 않을땐 0을 나타내는 지시함수 입니다.

#### Hard negative mining

고질적인 문제에 봉착했습니다. 각 grid cell마다 물체의 유무를 검사하기 때문에 배경의 수가 압도적으로 많기 때문입니다. 이것을 해결하기위해 각각의 Default Box에 대해서 confidence loss를 정렬하고 negative(배경)와 positive(물체)의 비율을 3:1이 되도록 뽑습니다. 이러한 방법으로 최적화를 빠르게 수행하고 학습도 안정적으로 진행했다고 설명하고 있습니다.

### 4. Result

SSD의 결과는 단연코 SOTA였습니다. two-stage method인 Faster R-CNN과 비교해보아도 mAP, FPS 모두 앞도했습니다. input size를 300x300으로 했을때대비 512x512로 했을때는 mAP는 무려 81.6%를 보여주었습니다. 이것은 input size를 키웠을 때, 작은 물체를 더욱 잘 탐지 한다는것을 보여주는 결과라고 할 수 있습니다.

<p align="center">
<img alt="result" src="https://www.dropbox.com/s/fsc33p75f924j1v/Result.PNG?raw=1">
</p>

이러한 결과는 PASCAL VOC2007 데이터셋 뿐만아니라 VOC2012, COCO데이터셋에서도 우수한 결과를 보여주었습니다.

### 5. Conclusion

SSD는 MultiBox Detector라는 개념을 도입하여 정확도와 속도 두가지를 모두잡은 해자모델임에는 틀림없습니다. 또한 각각의 Feature Map마다 Prediction을 한다는 컨셉은 이후 Feature Pyramid Network(FPN)에 영감을 주게됩니다. 현재에도 SSD는 종종 사용하는 모델이기 때문에, 조금 어렵더라도 완벽하게 알고 넘어가야 향후 나오는 모델을 이해하는데 어려움이 없을것입니다.