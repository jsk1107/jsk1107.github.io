---
layout: post
title: Object Detection [6] YoLo v2
excerpt: AI Paper Review_YoLo
comments: true
categories : [AI Paper Review, YoLo, You only Look once]
use_math: true
---

YoLo v1은 속도는 빠르지만 낮은 mAP때문에 많은 Detection알고리즘으로 부터 고통받고 있었습니다. 작은 물체나 겹쳐있는 물체는 잘 탐지하지 못하는 Localization Error가 상당히 높았고 recall도 만족스러운 수치는 아니었습니다. 한마디로 상용화를 위한 알고리즘으로는 낙제점이었습니다. 정확도를 높이기 위해서는 네트워크를 더 깊고 넓게 만드는 작업이 필요했지만, 이것은 연산비용을 매우 증가시키기는 문제가 존재했습니다. 그래서 저자는 과거의 작업들에서 나온 아이디어들을 모아모아서 적용해보기 시작했고, 그렇게 탄생한것이 YoLo v2 입니다.

### 1. Better

정확도를 높이기 위해 7가지 방법을 제시합니다.

#### Batch Normalization

Batch Normalization은 수렴을 상당히 향상시키는 효과가 있습니다. 모든 Conv Layer에 BN을 추가시켰으며 2% 이상의 mAP 향상을 이끌어냈습니다. 또한 dropout을 모두 제거하였습니다. ResNet에서도 확인했듯이 보통 BN Layer가 추가되면 Dropout과 weight initailize는 하지 않습니다.

#### High Resolution Classifier

보통 Detection에서 사용하는 분류기의 Pre-train으로 사용하는 데이터는 ImageNet을 사용합니다. 따라서 대부분의 모델의 초기 입력 해상도는 224x224 입니다. YoLo v1에서도 이러한 사실에 근거하여 classifier는 입력 해상도를 224x224로 학습을 진행하고 Detection에서는 448x448로 해상도를 높였습니다. classifier의 낮은 해상도는 mAP를 저하시키는 요인이 되므로, classification network를 448x448해상도로 변경하여 10 epochs동안 fine tuning을 해주었습니다. 이 방법을 통해 약 4%의 mAP 상승을 이끌어냈다고 합니다.

#### Convolutional With Anchor Boxes

YoLo v1에서는 FC Layer를 두고 좌표를 예측하는 방법을 사용했습니다. 하지만 Faster R-CNN이나 SSD와 같이 Anchor Box 개념을 도입하는것이 엄청난 속도향상을 가져온다는것이 확인되었고 YoLo v2에서도 Anchor의 개념을 도입합니다. 그러기 위해서 FC Layer를 모두 제거하고 해당 위치에 Conv Layer를 사용하고 해상도를 416x416으로 축소시킵니다. 일반적으로 큰 객체는 이미지의 중심을 차지하고 있는 경우가 대부분이기 때문에, 이미지의 중심 grid cell이 객체를 탐지할 의무를 가질 수 있는 cell로 만들기 위함입니다. 이러한 Anchor Box를 도입함으로써 0.3% mAP가 감소하였으나, Recall은 무려 7%나 상승하였습니다.

<p align="center">
<img width="450" alt="yolo_part1" src="https://www.dropbox.com/s/vv0ykesoa709ps1/yolo_part.png?raw=1">
</p>

개인적인 생각으로는 이 부분은 일반화의 오류이지 않을까 생각합니다. 인물중심의 사진이라면 피사체가 이미지의 중심에 들어올 확률을 높겠지만 대부분의 사진은 그렇지 않은 경우가 대부분이기 때문이죠. 굳이 해상도를 416으로 만들 필요가 있었을까 합니다.

#### Dimension Clusters

그렇다면 Anchor Box를 어떻게 생성할것인가? Faster R-CNN, SSD에서는 사용자가 직접 선택한 Anchor Box를 사용하였지만 YoLo v2에서는 이러한 부분도 데이터에 depend되도록 Anchor Box를 조정하였습니다.

<p align="center">
<img width="400" alt="clustering" src="https://www.dropbox.com/s/l52i3vvhlobz7pw/yolo_clustering.PNG?raw=1">
</p>

바로 k-means clustering을 활용하여 객체들간의 Bounding Box를 군집화 시키겠다는 것입니다. 즉, training 데이터셋의 Ground Truth Box를 모두 뜯어모아서 군집화를 하겠다는 의미입니다. 일반적인 k-means의 공식을 사용하는것은(즉, 중심좌표를 이용한 유클리디안 거리) 큰 Bounding Box에서 에러를 향상시킨다고 합니다. 그래서 유클리디안 거리대신 저자가 직접 설계한 metric을 이용합니다.

$$
    d(box, centroid) = 1 - IOU(box, centroid)
$$

cluster의 갯수 k도 조정하며 실험을 진행하였는데 k=5일때 가장 이상적인 정확도와 속도를 가졌다고 설명하고 있습니다.

저는 이부분을 이해하는데 상당한 시간을 소모했습니다. 아니 도대체 Bounding Box를 군집화한다는게 무슨말인거지? centroid는 뭐지?? 이부분은 코드를 뜯어보고나서 이해할 수 있었습니다.

<p align="center">
<img width="400" alt="yolo_kmeans" src="https://www.dropbox.com/s/ac7dg2m07a559d6/yolo_kmeans.PNG?raw=1">
</p>

위 코드는 [keras](https://github.com/experiencor/keras-yolo2/blob/master/gen_anchors.py#L66)로 설계되어있는 코드중 일부입니다. 인자로 ann_dims(Type은 튜플)를 받는데 이것은 train 데이터셋의 모든 Ground Truth Box의 width, height 정보를 가지고 있습니다. Line73를 보면 해당 튜플에서 랜덤으로 ``anchor num``만큼 indice를 설정합니다. Line74에서 ann_dims에서 indice의 인덱스를 가지고와 centroid를 초기화합니다. centroid는 ann_dims로 부터 얻어왔으므로 해당 Anchor Box가 될 weigth, height라는 사실을 알 수 있습니다. 초기화된 centroid는 ann_dims와 IOU비교하여 수렴할때까지 반복처리하면 5개의 Anchor Box의 width, height를 얻을 수 있습니다. 즉, 작은 객체를 대표하는 Anchor Box에서부터 점점 크기를 키워나가며, 가장 큰 객체를 대표하는 Anchor Box까지 5개의 Anchor Box를 만드는것입니다. 

논문에서는 train 데이터셋으로 VOC와 COCO를 사용하였으므로, 5개의 군집은 위 데이터셋에 최적화된 Anchor Box가 됩니다. 따라서 Custom Dataset을 사용하고자 한다면 Anchor Box를 다시 구해주는것이 보다 mAP를 올릴 수 있을것입니다.

#### Direct location prediction

Faster R-CNN이나 SSD에서 사용하는 Box Regression의 방식은 별다른 제약조건이 없기때문에 초기에 예측값이 불안정하다는 특징이 있습니다.

$$
    x = (t_{x} \ast w_{a}) - x_{a}\\
    y = (t_{y} \ast h_{a}) - y_{a}
$$

좌표의 중심 (x, y)는 위와 같은 식에 의해서 계산됩니다. prediction된 값이 $t_{x}, t_{y}$는 중심좌표가 이동하는 거리를 나타내지요. 예를들어 $t_{x}=1$ 이면, 예측된 Bounding Box가 오른쪽으로 한칸 이동하고 $t_{x}=-1$ 이면 왼쪽으로 한칸 이동하게됩니다. 이렇게 되면 Bounding Box의 중심이 random하게 위치하게 되버립니다.

이러한 경우를 제거하고자 YoLo v1에서 사용했던 방법을 사용합니다. grid cell의 중심에 위치하도록 0~1의 값으로 만드는것입니다. YoLo v1에서는 정규화를 통해서 0~1 사이의 값으로 만들어주었으나, 이번에는 로지스틱 활성함수를 활용하여 생성하게 됩니다. 이런 관점으로 Bounding Box를 예측하는 방법은 아래와 같습니다.

$$
    b_{x} = \sigma(t_{x}) + c_{x}\\
    b_{y} = \sigma(t_{y}) + c_{y}\\
    b_{w} = p_{w}e^{t_{w}}\\
    b_{h} = p_{w}e^{t_{w}}\\
    Pr(object) \ast IOU(b, object) = \sigma(t_{o})
$$

Box Regressor는 기존 방법과 동일하고 단지 $t_{x}, t_{y}$에 로지스틱 활성함수를 적용한 형태가 되겠습니다. 또한 $c_{x}, c_{y}$는 target이 되는 grid cell의 좌상단 좌표가 됩니다. 즉 위의 개 사진으로 예시를 들면 가운데 target이 되는 grid cell(빨간영역)에서 좌표의 예측값을 획득하게 됩니다. 만약 예측된 Bounding Box의 예측값 $(t_{x}, t_{y}) = (0.3, 0.4)$ 라고 한다면, $b_{x}=0.3+6, b_{y}=0.4+6$ 이 되게 됩니다. $p_{w}, p_{h}$ 는 Dimension Clusters로 획득한 5개의 Anchor Box들의 w, h가 되며, 마찬가지로 예측된 Bounding Box의 $t_{w}, t_{h}$ 으로 간단히 계산할 수 있습니다.  

<p align="center">
<img width="400" alt="bounding box" src="https://www.dropbox.com/s/th0pcdaehsgj52c/yolo_boundingbox.PNG?raw=1">
</p>

이러한 제약조건을 추가시킴에 따라, Parameter가 더욱 빨리 수렴하였으며 안정적인 네트워크를 설계할 수 있습니다.

#### Fine-Grained Features

416x416 크기의 이미지를 입력받았을때 최종 Feature map의 크기는 13x13입니다. 하지만 이정도의 grid cell로는 작은 이미지를 탐지하는데 어려움이 있다는 사실을 SSD의 사례를 통해서 확인했었습니다.

SSD는 다양한 Feature Map으로부터 예측을 진행하는 반면, YoLo v2는 조금 다양한 방법을 시도합니다. 맨 마지막 Feature Map과 바로 전에 등장하는 Feature Map을 연결하는 방식입니다. 이것은 마치 ResNet에서 Indentity Mapping과 유사한 방법입니다. 만약 최종 이전의 Feature Map의 크기가 26x26x512 라면, 이것을 13x13x2048 로 크기를 변환합니다. 그러면 최종 Feature Map과 연결할 수 있습니다. 이 방법은 이전 Feature Map으로부터 특징값에 대한 정보를 연결한다는 의미로써 passthurough layer라고 부른다고 합니다. 이러한 방법을 통해 약 1%의 향상을 이루었습니다.

#### Multi-Scale Training

YoLo v2에서는 v1과는 다르게 FC layer를 제거하고 Conv layer를 추가하였습니다. 따라서 네트워크를 Fully Conv Network가 되어 입력이미지의 크기에 영향을 받지 않고, Feature Map을 자유롭게 조절할 수 있다는 장점이 있습니다. 이러한 장점에 착안하여 다양한 크기를 가지는 이미지에 대해서도 학습이 가능하도록 구성하였습니다.

10 Batch마다 랜덤하게 이미지의 크기를 정합니다. 네트워크 구조상 한번의 downsampling당 32배씩 축소하기때문에 입력이미지의 크기를 32배수만큼 조절하여 입력합니다. 즉, {320, 320+32, 320+32*2, ... , 320+32*9}. 최소 입력크기는 320이고 최대입력 크기는 608이 되도록 사이범위에서 32배수만큼 조절하는 방법으로 다양한 이미지 크기를 입력으로 받습니다. 

이렇게 위와같은 7가지 방법을 사용하였을때, 다음과 같은 놀라운 결과를 보여주게 됩니다.

<p align="center">
<img width="400" alt="result" src="https://www.dropbox.com/s/h0yp26ospsukuj3/yolo_resuslt.PNG?raw=1">
</p>

### 2. Faster

대부분의 Detection 모델이 Backbone으로 차용하고 있는 모델은 VGG-16입니다. 좋은 모델이기는 하지만 30억번이 넘는 연산을 수행해야하고 Parameter의 갯수는 1억3천만개에 육박합니다. GoogleNet으로 알려져있는 Inception 또한 약 8억번의 연산과 500만~2200만개의 Parameter가 존재합니다. 이러한 막대한 연산비용은 속도저하에 지대한 영향을 끼칩니다.

#### DarkNet-19

그래서 등장하게 된것이 DarkNet-19입니다. 해당모델은 19개의 Conv layer가 있으며 5개의 Maxpooling layer를 가지고있습니다. 

<p align="center">
<img width="400" alt="darknet19" src="https://www.dropbox.com/s/7ricn51egwbtlqx/yolo_darknet.PNG?raw=1">
</p>

연산횟수도 약 5억번으로 감소하였고, ImageNet 데이터셋으로 실험한 결과 72.9% top-1 acc, 91.2% top-5 acc를 기록하여 연산비용은 줄이고 정확도는 높이는 모델을 개발하게됩니다. 해당 Backbone은 YoLo v3에서도 지속적으로 사용되게 됩니다.

### 3. Stronger

해당 부분은 Classification에 대하여 여러 데이터를 계층적으로 연결하는 내용에 대한것입니다. 요약하자면 **새**라는 종류의 하위 카테고리로 황새, 두루미, 철새 등의 카테고리를 계층적인 Class로 바라보고 조건부 확률을 이용한 Multi-Labeling을 통해 classification을 하겠다 라는 내용입니다. 내용이 Detection과는 관련성이 떨어지기 때문에 더이상의 내용은 생략하겠습니다.

### 4. Conclusion

YoLo v2는 한마디로 v1 이후로 등장한 좋은 방법론을 다 끌어모은 모델이다 라고 평가할 수 있겠습니다. 마지막 Stronger 부분에서 주장하는 바는 큰 이슈를 끌지 못하였고 Better, Faster 부분에서도 특별한점을 발견하기는 어려웠습니다. 단순히 Anchor Box를 도입하고, Backbone의 네트워크 구조를 수정한 정도니까요. 그래서인지 YoLo v2는 SSD에 밀려 큰 주목을 받지는 못하였습니다. 하지만 당시에 SOTA임에는 분명했고, 빠른 속도의 장점으로 인해 실시간 탐지 영역에서는 많이 사용되기도 하였습니다.